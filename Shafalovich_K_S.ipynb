{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PknKdZDv3wlz",
        "outputId": "f674aa5c-03e8-4e3f-956d-e090230cc995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (17040, 785)\n",
            "Train data columns: Index(['label', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n",
            "       'pixel7', 'pixel8', 'pixel9',\n",
            "       ...\n",
            "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
            "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
            "      dtype='object', length=785)\n",
            "Test data shape: (10000, 784)\n",
            "Test data columns: Index(['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7',\n",
            "       'pixel8', 'pixel9', 'pixel10',\n",
            "       ...\n",
            "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
            "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
            "      dtype='object', length=784)\n",
            "Train data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17040 entries, 0 to 17039\n",
            "Columns: 785 entries, label to pixel784\n",
            "dtypes: float64(424), int64(361)\n",
            "memory usage: 102.1 MB\n",
            "None\n",
            "Test data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Columns: 784 entries, pixel1 to pixel784\n",
            "dtypes: int64(784)\n",
            "memory usage: 59.8 MB\n",
            "None\n",
            "Missing values in train data:\n",
            "label       0\n",
            "pixel1      0\n",
            "pixel2      0\n",
            "pixel3      0\n",
            "pixel4      0\n",
            "           ..\n",
            "pixel780    0\n",
            "pixel781    0\n",
            "pixel782    0\n",
            "pixel783    0\n",
            "pixel784    0\n",
            "Length: 785, dtype: int64\n",
            "Missing values in test data:\n",
            "pixel1      0\n",
            "pixel2      0\n",
            "pixel3      0\n",
            "pixel4      0\n",
            "pixel5      0\n",
            "           ..\n",
            "pixel780    0\n",
            "pixel781    0\n",
            "pixel782    0\n",
            "pixel783    0\n",
            "pixel784    0\n",
            "Length: 784, dtype: int64\n",
            "Train data columns: Index(['label', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n",
            "       'pixel7', 'pixel8', 'pixel9',\n",
            "       ...\n",
            "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
            "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
            "      dtype='object', length=785)\n",
            "Test data columns: Index(['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7',\n",
            "       'pixel8', 'pixel9', 'pixel10',\n",
            "       ...\n",
            "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
            "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
            "      dtype='object', length=784)\n",
            "First few rows of train data:\n",
            "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
            "0      2       0       0       0       0       0       0       0       0   \n",
            "1      9       0       0       0       0       0       0       0       0   \n",
            "2      6       0       0       0       0       0       0       0       5   \n",
            "3      0       0       0       0       1       2       0       0       0   \n",
            "4      3       0       0       0       0       0       0       0       0   \n",
            "\n",
            "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
            "0       0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "1       0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "2       0  ...       0.0       0.0       0.0      30.0      43.0       0.0   \n",
            "3       0  ...       3.0       0.0       0.0       0.0       0.0       1.0   \n",
            "4       0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
            "\n",
            "   pixel781  pixel782  pixel783  pixel784  \n",
            "0       0.0       0.0       0.0       0.0  \n",
            "1       0.0       0.0       0.0       0.0  \n",
            "2       0.0       0.0       0.0       0.0  \n",
            "3       0.0       0.0       0.0       0.0  \n",
            "4       0.0       0.0       0.0       0.0  \n",
            "\n",
            "[5 rows x 785 columns]\n",
            "First few rows of test data:\n",
            "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
            "0       0       0       0       0       0       0       0       9       8   \n",
            "1       0       0       0       0       0       0       0       0       0   \n",
            "2       0       0       0       0       0       0      14      53      99   \n",
            "3       0       0       0       0       0       0       0       0       0   \n",
            "4       0       0       0       0       0       0       0       0       0   \n",
            "\n",
            "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
            "0        0  ...       103        87        56         0         0         0   \n",
            "1        0  ...        34         0         0         0         0         0   \n",
            "2       17  ...         0         0         0         0        63        53   \n",
            "3      161  ...       137       126       140         0       133       224   \n",
            "4        0  ...         0         0         0         0         0         0   \n",
            "\n",
            "   pixel781  pixel782  pixel783  pixel784  \n",
            "0         0         0         0         0  \n",
            "1         0         0         0         0  \n",
            "2        31         0         0         0  \n",
            "3       222        56         0         0  \n",
            "4         0         0         0         0  \n",
            "\n",
            "[5 rows x 784 columns]\n",
            "Original x_train shape: (17040, 784)\n",
            "Original x_test shape: (10000, 784)\n",
            "x_train shape: (17040, 28, 28, 1)\n",
            "y_train shape: (17040,)\n",
            "x_test shape: (10000, 28, 28, 1)\n",
            "y = 4 Coat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train data - 17040\n",
            "Number of test data - 10000\n",
            "x_train shape: (12040, 28, 28, 1) y_train shape: (12040, 10)\n",
            "12040 train set\n",
            "5000 validation set\n",
            "10000 test set\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Загрузка данных из CSV файлов\n",
        "train_data = pd.read_csv('fmnist_train.csv')\n",
        "test_data = pd.read_csv('fmnist_test.csv')\n",
        "\n",
        "# Заполнение пропущенных значений\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)\n",
        "\n",
        "# Предварительная обработка данных\n",
        "train_data.drop(columns=['Id'], inplace=True)\n",
        "test_data.drop(columns=['Id'], inplace=True)\n",
        "\n",
        "# Проверка формы и названий столбцов в обучающих данных\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Train data columns:\", train_data.columns)\n",
        "\n",
        "# Проверка формы и названий столбцов в тестовых данных\n",
        "print(\"Test data shape:\", test_data.shape)\n",
        "print(\"Test data columns:\", test_data.columns)\n",
        "\n",
        "# Проведение первичного анализа данных\n",
        "print(\"Train data info:\")\n",
        "print(train_data.info())\n",
        "print(\"Test data info:\")\n",
        "print(test_data.info())\n",
        "\n",
        "# Проверка на наличие NaN значений\n",
        "print(\"Missing values in train data:\")\n",
        "print(train_data.isnull().sum())\n",
        "print(\"Missing values in test data:\")\n",
        "print(test_data.isnull().sum())\n",
        "\n",
        "# Проверка названий столбцов\n",
        "print(\"Train data columns:\", train_data.columns)\n",
        "print(\"Test data columns:\", test_data.columns)\n",
        "\n",
        "# Проверка первых строк данных\n",
        "print(\"First few rows of train data:\")\n",
        "print(train_data.head())\n",
        "print(\"First few rows of test data:\")\n",
        "print(test_data.head())\n",
        "\n",
        "\n",
        "# Разделение данных на изображения, метки и id\n",
        "if 'id' in train_data.columns:\n",
        "    y_train = train_data['label'].values\n",
        "    x_train = train_data.drop(['label', 'id'], axis=1).values\n",
        "else:\n",
        "    y_train = train_data['label'].values\n",
        "    x_train = train_data.drop(['label'], axis=1).values\n",
        "\n",
        "if 'id' in test_data.columns:\n",
        "    x_test = test_data.drop(['id'], axis=1).values\n",
        "else:\n",
        "    x_test = test_data.values\n",
        "\n",
        "# Проверка формы данных перед преобразованием\n",
        "print(\"Original x_train shape:\", x_train.shape)\n",
        "print(\"Original x_test shape:\", x_test.shape)\n",
        "\n",
        "\n",
        "# Преобразование данных в необходимую форму\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "\n",
        "# Определение текстовых меток классов\n",
        "fashion_mnist_labels = [\"T-shirt/top\",  # индекс 0\n",
        "                        \"Trouser\",      # индекс 1\n",
        "                        \"Pullover\",     # индекс 2\n",
        "                        \"Dress\",        # индекс 3\n",
        "                        \"Coat\",         # индекс 4\n",
        "                        \"Sandal\",       # индекс 5\n",
        "                        \"Shirt\",        # индекс 6\n",
        "                        \"Sneaker\",      # индекс 7\n",
        "                        \"Bag\",          # индекс 8\n",
        "                        \"Ankle boot\"]   # индекс 9\n",
        "\n",
        "# Индекс изображения\n",
        "img_index = 5\n",
        "# y_train содержит метки, варьирующиеся от 0 до 9\n",
        "label_index = y_train[img_index]\n",
        "# Печать метки, например, 2 Pullover\n",
        "print(\"y = \" + str(label_index) + \" \" + (fashion_mnist_labels[label_index]))\n",
        "# Показ одного из изображений из тренировочного набора\n",
        "plt.imshow(x_train[img_index].reshape(28, 28), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# Нормализация данных изображений\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "print(\"Number of train data - \" + str(len(x_train)))\n",
        "print(\"Number of test data - \" + str(len(x_test)))\n",
        "\n",
        "# Разделение тренировочных данных на тренировочный и валидационный наборы (5000 на валидацию, 55000 на тренировку)\n",
        "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
        "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
        "\n",
        "# One-hot кодирование меток\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "\n",
        "# Печать формы тренировочного набора\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Печать количества тренировочных, валидационных и тестовых данных\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_valid.shape[0], 'validation set')\n",
        "print(x_test.shape[0], 'test set')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание модели Sequential\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Определение входной формы в первом слое нейронной сети\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Печать структуры модели\n",
        "model.summary()\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Использование ModelCheckpoint для сохранения лучших весов модели\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.keras', verbose=1, save_best_only=True)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_valid shape:\", x_valid.shape)\n",
        "print(\"y_valid shape:\", y_valid.shape)\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=42,  # текущий batch size\n",
        "          epochs=84,\n",
        "          validation_data=(x_valid, y_valid),\n",
        "          callbacks=[checkpointer])\n",
        "\n",
        "# Загрузка весов с лучшей валидационной точностью\n",
        "model.load_weights('model.weights.best.keras')\n",
        "\n",
        "# Оценка модели на тестовом наборе\n",
        "score = model.evaluate(x_test, verbose=0)\n",
        "\n",
        "# Печать точности на тестовом наборе\n",
        "print('\\n', 'Test accuracy:', score[1])\n",
        "\n",
        "# Предсказание на тестовых данных\n",
        "y_hat = model.predict(x_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_80DliHb3_39",
        "outputId": "9548c645-7efa-4c52-92da-30cde0e73d12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 28, 28, 64)        320       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 14, 14, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 14, 14, 32)        8224      \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 7, 7, 32)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 7, 7, 32)          0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 1568)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               401664    \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 412778 (1.57 MB)\n",
            "Trainable params: 412778 (1.57 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "x_train shape: (12040, 28, 28, 1)\n",
            "y_train shape: (12040, 10)\n",
            "x_valid shape: (5000, 28, 28, 1)\n",
            "y_valid shape: (5000, 10)\n",
            "Epoch 1/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.8922 - accuracy: 0.6620\n",
            "Epoch 1: val_loss improved from inf to 0.54342, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 20s 65ms/step - loss: 0.8922 - accuracy: 0.6620 - val_loss: 0.5434 - val_accuracy: 0.7938\n",
            "Epoch 2/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.5668 - accuracy: 0.7883\n",
            "Epoch 2: val_loss improved from 0.54342 to 0.46558, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.5668 - accuracy: 0.7883 - val_loss: 0.4656 - val_accuracy: 0.8212\n",
            "Epoch 3/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.4972 - accuracy: 0.8119\n",
            "Epoch 3: val_loss improved from 0.46558 to 0.43366, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.4975 - accuracy: 0.8116 - val_loss: 0.4337 - val_accuracy: 0.8480\n",
            "Epoch 4/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.4493 - accuracy: 0.8318\n",
            "Epoch 4: val_loss improved from 0.43366 to 0.37417, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.4491 - accuracy: 0.8316 - val_loss: 0.3742 - val_accuracy: 0.8612\n",
            "Epoch 5/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.4321 - accuracy: 0.8378\n",
            "Epoch 5: val_loss improved from 0.37417 to 0.35410, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.4320 - accuracy: 0.8378 - val_loss: 0.3541 - val_accuracy: 0.8690\n",
            "Epoch 6/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8503\n",
            "Epoch 6: val_loss did not improve from 0.35410\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.4029 - accuracy: 0.8503 - val_loss: 0.3561 - val_accuracy: 0.8698\n",
            "Epoch 7/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.3916 - accuracy: 0.8567\n",
            "Epoch 7: val_loss improved from 0.35410 to 0.35152, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.3912 - accuracy: 0.8567 - val_loss: 0.3515 - val_accuracy: 0.8720\n",
            "Epoch 8/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.8613\n",
            "Epoch 8: val_loss improved from 0.35152 to 0.33148, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.3739 - accuracy: 0.8613 - val_loss: 0.3315 - val_accuracy: 0.8810\n",
            "Epoch 9/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.3565 - accuracy: 0.8660\n",
            "Epoch 9: val_loss did not improve from 0.33148\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.3565 - accuracy: 0.8660 - val_loss: 0.3351 - val_accuracy: 0.8766\n",
            "Epoch 10/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8657\n",
            "Epoch 10: val_loss improved from 0.33148 to 0.31351, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 19s 66ms/step - loss: 0.3474 - accuracy: 0.8657 - val_loss: 0.3135 - val_accuracy: 0.8880\n",
            "Epoch 11/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.3328 - accuracy: 0.8712\n",
            "Epoch 11: val_loss did not improve from 0.31351\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.3327 - accuracy: 0.8712 - val_loss: 0.3231 - val_accuracy: 0.8822\n",
            "Epoch 12/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8761\n",
            "Epoch 12: val_loss did not improve from 0.31351\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.3260 - accuracy: 0.8760 - val_loss: 0.3182 - val_accuracy: 0.8848\n",
            "Epoch 13/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8836\n",
            "Epoch 13: val_loss improved from 0.31351 to 0.30144, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.3130 - accuracy: 0.8836 - val_loss: 0.3014 - val_accuracy: 0.8900\n",
            "Epoch 14/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.8878\n",
            "Epoch 14: val_loss did not improve from 0.30144\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.3012 - accuracy: 0.8878 - val_loss: 0.3066 - val_accuracy: 0.8888\n",
            "Epoch 15/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.8846\n",
            "Epoch 15: val_loss improved from 0.30144 to 0.29133, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.3075 - accuracy: 0.8846 - val_loss: 0.2913 - val_accuracy: 0.8992\n",
            "Epoch 16/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2936 - accuracy: 0.8900\n",
            "Epoch 16: val_loss improved from 0.29133 to 0.28423, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.2936 - accuracy: 0.8900 - val_loss: 0.2842 - val_accuracy: 0.9014\n",
            "Epoch 17/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.8961\n",
            "Epoch 17: val_loss improved from 0.28423 to 0.28093, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.2811 - accuracy: 0.8961 - val_loss: 0.2809 - val_accuracy: 0.9028\n",
            "Epoch 18/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.8968\n",
            "Epoch 18: val_loss did not improve from 0.28093\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.2723 - accuracy: 0.8968 - val_loss: 0.2950 - val_accuracy: 0.8914\n",
            "Epoch 19/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.8952\n",
            "Epoch 19: val_loss did not improve from 0.28093\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.2710 - accuracy: 0.8952 - val_loss: 0.2866 - val_accuracy: 0.8964\n",
            "Epoch 20/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9001\n",
            "Epoch 20: val_loss did not improve from 0.28093\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.2684 - accuracy: 0.9001 - val_loss: 0.2845 - val_accuracy: 0.8992\n",
            "Epoch 21/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9035\n",
            "Epoch 21: val_loss did not improve from 0.28093\n",
            "287/287 [==============================] - 17s 58ms/step - loss: 0.2564 - accuracy: 0.9036 - val_loss: 0.2914 - val_accuracy: 0.8998\n",
            "Epoch 22/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9062\n",
            "Epoch 22: val_loss did not improve from 0.28093\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.2503 - accuracy: 0.9063 - val_loss: 0.2844 - val_accuracy: 0.9000\n",
            "Epoch 23/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9060\n",
            "Epoch 23: val_loss improved from 0.28093 to 0.27663, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.2473 - accuracy: 0.9060 - val_loss: 0.2766 - val_accuracy: 0.9052\n",
            "Epoch 24/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2368 - accuracy: 0.9081\n",
            "Epoch 24: val_loss did not improve from 0.27663\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.2365 - accuracy: 0.9082 - val_loss: 0.2791 - val_accuracy: 0.9050\n",
            "Epoch 25/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9102\n",
            "Epoch 25: val_loss did not improve from 0.27663\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.2377 - accuracy: 0.9100 - val_loss: 0.2820 - val_accuracy: 0.9044\n",
            "Epoch 26/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9120\n",
            "Epoch 26: val_loss did not improve from 0.27663\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.2277 - accuracy: 0.9120 - val_loss: 0.2800 - val_accuracy: 0.9068\n",
            "Epoch 27/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9108\n",
            "Epoch 27: val_loss improved from 0.27663 to 0.27250, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.2360 - accuracy: 0.9106 - val_loss: 0.2725 - val_accuracy: 0.9074\n",
            "Epoch 28/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9164\n",
            "Epoch 28: val_loss did not improve from 0.27250\n",
            "287/287 [==============================] - 18s 61ms/step - loss: 0.2167 - accuracy: 0.9164 - val_loss: 0.2937 - val_accuracy: 0.8998\n",
            "Epoch 29/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9202\n",
            "Epoch 29: val_loss did not improve from 0.27250\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.2129 - accuracy: 0.9201 - val_loss: 0.2797 - val_accuracy: 0.9066\n",
            "Epoch 30/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2144 - accuracy: 0.9170\n",
            "Epoch 30: val_loss did not improve from 0.27250\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.2144 - accuracy: 0.9169 - val_loss: 0.2770 - val_accuracy: 0.9064\n",
            "Epoch 31/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9195\n",
            "Epoch 31: val_loss did not improve from 0.27250\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.2115 - accuracy: 0.9193 - val_loss: 0.2740 - val_accuracy: 0.9088\n",
            "Epoch 32/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9209\n",
            "Epoch 32: val_loss improved from 0.27250 to 0.27224, saving model to model.weights.best.keras\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.2055 - accuracy: 0.9211 - val_loss: 0.2722 - val_accuracy: 0.9102\n",
            "Epoch 33/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9224\n",
            "Epoch 33: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 61ms/step - loss: 0.2067 - accuracy: 0.9224 - val_loss: 0.2747 - val_accuracy: 0.9026\n",
            "Epoch 34/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9270\n",
            "Epoch 34: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.1965 - accuracy: 0.9268 - val_loss: 0.2856 - val_accuracy: 0.9058\n",
            "Epoch 35/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9291\n",
            "Epoch 35: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.1887 - accuracy: 0.9291 - val_loss: 0.2751 - val_accuracy: 0.9048\n",
            "Epoch 36/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9255\n",
            "Epoch 36: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1953 - accuracy: 0.9255 - val_loss: 0.2813 - val_accuracy: 0.9058\n",
            "Epoch 37/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9267\n",
            "Epoch 37: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.1887 - accuracy: 0.9267 - val_loss: 0.2840 - val_accuracy: 0.9062\n",
            "Epoch 38/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.9311\n",
            "Epoch 38: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1834 - accuracy: 0.9311 - val_loss: 0.2812 - val_accuracy: 0.9102\n",
            "Epoch 39/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1807 - accuracy: 0.9322\n",
            "Epoch 39: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1807 - accuracy: 0.9321 - val_loss: 0.2854 - val_accuracy: 0.9052\n",
            "Epoch 40/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9295\n",
            "Epoch 40: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1819 - accuracy: 0.9294 - val_loss: 0.2896 - val_accuracy: 0.9062\n",
            "Epoch 41/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9289\n",
            "Epoch 41: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.1833 - accuracy: 0.9289 - val_loss: 0.2946 - val_accuracy: 0.9002\n",
            "Epoch 42/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9277\n",
            "Epoch 42: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.1808 - accuracy: 0.9277 - val_loss: 0.3097 - val_accuracy: 0.8982\n",
            "Epoch 43/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9355\n",
            "Epoch 43: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1735 - accuracy: 0.9355 - val_loss: 0.2974 - val_accuracy: 0.9032\n",
            "Epoch 44/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9340\n",
            "Epoch 44: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.1691 - accuracy: 0.9340 - val_loss: 0.2952 - val_accuracy: 0.9090\n",
            "Epoch 45/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9405\n",
            "Epoch 45: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1654 - accuracy: 0.9405 - val_loss: 0.3000 - val_accuracy: 0.9020\n",
            "Epoch 46/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9375\n",
            "Epoch 46: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.1630 - accuracy: 0.9375 - val_loss: 0.2940 - val_accuracy: 0.9080\n",
            "Epoch 47/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9391\n",
            "Epoch 47: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1631 - accuracy: 0.9390 - val_loss: 0.2992 - val_accuracy: 0.9092\n",
            "Epoch 48/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1637 - accuracy: 0.9399\n",
            "Epoch 48: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1634 - accuracy: 0.9400 - val_loss: 0.2939 - val_accuracy: 0.9086\n",
            "Epoch 49/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1524 - accuracy: 0.9414\n",
            "Epoch 49: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.1527 - accuracy: 0.9413 - val_loss: 0.2976 - val_accuracy: 0.9082\n",
            "Epoch 50/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9432\n",
            "Epoch 50: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1537 - accuracy: 0.9433 - val_loss: 0.3110 - val_accuracy: 0.9050\n",
            "Epoch 51/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9414\n",
            "Epoch 51: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.1527 - accuracy: 0.9414 - val_loss: 0.2995 - val_accuracy: 0.9048\n",
            "Epoch 52/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9418\n",
            "Epoch 52: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 58ms/step - loss: 0.1475 - accuracy: 0.9419 - val_loss: 0.2945 - val_accuracy: 0.9122\n",
            "Epoch 53/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9388\n",
            "Epoch 53: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1608 - accuracy: 0.9387 - val_loss: 0.3006 - val_accuracy: 0.9066\n",
            "Epoch 54/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9404\n",
            "Epoch 54: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1517 - accuracy: 0.9403 - val_loss: 0.3019 - val_accuracy: 0.9112\n",
            "Epoch 55/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9471\n",
            "Epoch 55: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 61ms/step - loss: 0.1449 - accuracy: 0.9472 - val_loss: 0.3036 - val_accuracy: 0.9066\n",
            "Epoch 56/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9400\n",
            "Epoch 56: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.1569 - accuracy: 0.9400 - val_loss: 0.2974 - val_accuracy: 0.9102\n",
            "Epoch 57/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9438\n",
            "Epoch 57: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1442 - accuracy: 0.9438 - val_loss: 0.3012 - val_accuracy: 0.9100\n",
            "Epoch 58/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9465\n",
            "Epoch 58: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1384 - accuracy: 0.9465 - val_loss: 0.2970 - val_accuracy: 0.9102\n",
            "Epoch 59/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.9463\n",
            "Epoch 59: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1393 - accuracy: 0.9463 - val_loss: 0.3182 - val_accuracy: 0.9018\n",
            "Epoch 60/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9453\n",
            "Epoch 60: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1386 - accuracy: 0.9453 - val_loss: 0.3108 - val_accuracy: 0.9092\n",
            "Epoch 61/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9456\n",
            "Epoch 61: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 66ms/step - loss: 0.1400 - accuracy: 0.9456 - val_loss: 0.3025 - val_accuracy: 0.9076\n",
            "Epoch 62/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9506\n",
            "Epoch 62: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.1323 - accuracy: 0.9506 - val_loss: 0.3175 - val_accuracy: 0.9062\n",
            "Epoch 63/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9487\n",
            "Epoch 63: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1372 - accuracy: 0.9487 - val_loss: 0.3026 - val_accuracy: 0.9118\n",
            "Epoch 64/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9484\n",
            "Epoch 64: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1355 - accuracy: 0.9484 - val_loss: 0.3089 - val_accuracy: 0.9096\n",
            "Epoch 65/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9521\n",
            "Epoch 65: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1295 - accuracy: 0.9521 - val_loss: 0.3091 - val_accuracy: 0.9086\n",
            "Epoch 66/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9463\n",
            "Epoch 66: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1380 - accuracy: 0.9463 - val_loss: 0.3167 - val_accuracy: 0.9098\n",
            "Epoch 67/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9498\n",
            "Epoch 67: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 66ms/step - loss: 0.1322 - accuracy: 0.9498 - val_loss: 0.3148 - val_accuracy: 0.9114\n",
            "Epoch 68/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9545\n",
            "Epoch 68: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.1250 - accuracy: 0.9545 - val_loss: 0.3137 - val_accuracy: 0.9098\n",
            "Epoch 69/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9511\n",
            "Epoch 69: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1292 - accuracy: 0.9511 - val_loss: 0.3148 - val_accuracy: 0.9094\n",
            "Epoch 70/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9485\n",
            "Epoch 70: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1342 - accuracy: 0.9485 - val_loss: 0.3161 - val_accuracy: 0.9090\n",
            "Epoch 71/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9502\n",
            "Epoch 71: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 66ms/step - loss: 0.1286 - accuracy: 0.9502 - val_loss: 0.3136 - val_accuracy: 0.9092\n",
            "Epoch 72/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9524\n",
            "Epoch 72: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.1252 - accuracy: 0.9525 - val_loss: 0.3113 - val_accuracy: 0.9122\n",
            "Epoch 73/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1142 - accuracy: 0.9570\n",
            "Epoch 73: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.1140 - accuracy: 0.9571 - val_loss: 0.3298 - val_accuracy: 0.9060\n",
            "Epoch 74/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9552\n",
            "Epoch 74: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1219 - accuracy: 0.9552 - val_loss: 0.3125 - val_accuracy: 0.9072\n",
            "Epoch 75/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9561\n",
            "Epoch 75: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 63ms/step - loss: 0.1157 - accuracy: 0.9561 - val_loss: 0.3371 - val_accuracy: 0.9068\n",
            "Epoch 76/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9561\n",
            "Epoch 76: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1158 - accuracy: 0.9561 - val_loss: 0.3267 - val_accuracy: 0.9104\n",
            "Epoch 77/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9518\n",
            "Epoch 77: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.1236 - accuracy: 0.9517 - val_loss: 0.3303 - val_accuracy: 0.9068\n",
            "Epoch 78/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9542\n",
            "Epoch 78: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 59ms/step - loss: 0.1219 - accuracy: 0.9542 - val_loss: 0.3292 - val_accuracy: 0.9048\n",
            "Epoch 79/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9550\n",
            "Epoch 79: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1153 - accuracy: 0.9550 - val_loss: 0.3394 - val_accuracy: 0.9096\n",
            "Epoch 80/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9571\n",
            "Epoch 80: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1150 - accuracy: 0.9571 - val_loss: 0.3361 - val_accuracy: 0.9072\n",
            "Epoch 81/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9551\n",
            "Epoch 81: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1242 - accuracy: 0.9551 - val_loss: 0.3299 - val_accuracy: 0.9058\n",
            "Epoch 82/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9556\n",
            "Epoch 82: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 19s 65ms/step - loss: 0.1159 - accuracy: 0.9555 - val_loss: 0.3230 - val_accuracy: 0.9060\n",
            "Epoch 83/84\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9559\n",
            "Epoch 83: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 17s 60ms/step - loss: 0.1194 - accuracy: 0.9559 - val_loss: 0.3292 - val_accuracy: 0.9112\n",
            "Epoch 84/84\n",
            "286/287 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9575\n",
            "Epoch 84: val_loss did not improve from 0.27224\n",
            "287/287 [==============================] - 18s 64ms/step - loss: 0.1127 - accuracy: 0.9576 - val_loss: 0.3436 - val_accuracy: 0.9124\n",
            "\n",
            " Test accuracy: 0.0\n",
            "313/313 [==============================] - 4s 12ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Предобработка тестовых данных\n",
        "# Нормализация данных изображений\n",
        "X_test = test_data.astype('float32') / 255\n",
        "\n",
        "# Преобразование формы тестовых данных\n",
        "X_test = X_test.values.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Предсказания на тестовом наборе данных\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Преобразование предсказаний в метки классов\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Создание DataFrame для результатов\n",
        "submission_df = pd.DataFrame({'Id': range(len(predicted_labels)), 'Label': predicted_labels})\n",
        "\n",
        "# Сохранение результатов в файле\n",
        "submission_df.to_csv('submission6.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPV3g5K_HJTC",
        "outputId": "225f976a-7a10-4a35-b8f5-1cf7fc1bca2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 10ms/step\n"
          ]
        }
      ]
    }
  ]
}